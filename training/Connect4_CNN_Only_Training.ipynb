{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connect 4 - CNN-Only Training (v2)\n",
        "\n",
        "**Project deliverables**: Train a **CNN (v2)** for 7-class move classification.\n",
        "- Path auto-detect: Colab (Drive) or local `Connect4_Combined`\n",
        "- On-the-fly horizontal flip augmentation\n",
        "- CNN architecture + warmup/cosine LR + optional Phase 2 fine-tune\n",
        "- Saves: `connect4_cnn_v2_best.keras`, `connect4_cnn_v2_final.keras`, and compatibility alias `connect4_cnn_final.keras`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup, Paths & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Mount Drive (Colab only)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Path auto-detect: Colab vs local\n",
        "if os.path.exists('/content/drive/MyDrive/Connect4_Combined'):\n",
        "    BASE = '/content/drive/MyDrive/Connect4_Combined'\n",
        "else:\n",
        "    BASE = os.path.join(os.getcwd(), 'Connect4_Combined')\n",
        "\n",
        "COMBINED_DATASET = f'{BASE}/datasets/connect4_combined_unique.npz'\n",
        "MODEL_DIR = f'{BASE}/models'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Configurable parameters\n",
        "TARGET_VAL_ACC = 0.72\n",
        "EPOCHS_PHASE1 = 60\n",
        "EPOCHS_FINETUNE = 10\n",
        "BATCH_SIZE = 256\n",
        "INIT_LR = 1e-3\n",
        "FINETUNE_LR = 1e-5\n",
        "WARMUP_EPOCHS = 3\n",
        "EARLY_STOP_PATIENCE = 10\n",
        "AUG_FLIP_PROB = 0.5\n",
        "SEED = 42\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print('Loading dataset...')\n",
        "npz = np.load(COMBINED_DATASET)\n",
        "X_train = npz['X_train']\n",
        "y_train = npz['y_train']\n",
        "X_val = npz['X_val']\n",
        "y_val = npz['y_val']\n",
        "X_test = npz['X_test']\n",
        "y_test = npz['y_test']\n",
        "print(f'Train: {X_train.shape[0]:,} | Val: {X_val.shape[0]:,} | Test: {X_test.shape[0]:,}')\n",
        "print(f'Model dir: {MODEL_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Pipeline (augmentation + one-hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_and_one_hot(x, y):\n",
        "    \"\"\"On-the-fly horizontal flip (AUG_FLIP_PROB) + one-hot labels.\"\"\"\n",
        "    if tf.random.uniform(()) < AUG_FLIP_PROB:\n",
        "        x = tf.image.flip_left_right(x)\n",
        "        y = 6 - y\n",
        "    return x, tf.one_hot(tf.cast(y, tf.int32), NUM_CLASSES)\n",
        "\n",
        "def to_one_hot(x, y):\n",
        "    return x, tf.one_hot(tf.cast(y, tf.int32), NUM_CLASSES)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_ds = train_ds.shuffle(20000, seed=SEED).map(augment_and_one_hot, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_ds = val_ds.map(to_one_hot, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_ds = test_ds.map(to_one_hot, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "print('Data pipeline ready (augmentation on train only)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mixed Precision & LR Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print('Mixed precision enabled')\n",
        "\n",
        "def warmup_cosine_lr(epoch, lr):\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        return INIT_LR * (epoch + 1) / WARMUP_EPOCHS\n",
        "    progress = (epoch - WARMUP_EPOCHS) / max(1, EPOCHS_PHASE1 - WARMUP_EPOCHS)\n",
        "    return FINETUNE_LR + 0.5 * (INIT_LR - FINETUNE_LR) * (1 + np.cos(np.pi * progress))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build CNN (10 res blocks, 256 filters, BatchNorm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L2_REG = 1e-4\n",
        "\n",
        "def res_block(x, filters=256):\n",
        "    shortcut = x\n",
        "    x = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=keras.regularizers.l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=keras.regularizers.l2(L2_REG))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    return layers.ReLU()(x)\n",
        "\n",
        "inputs = keras.Input(shape=(6, 7, 2))\n",
        "x = layers.Conv2D(256, 3, padding='same', kernel_regularizer=keras.regularizers.l2(L2_REG))(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "for _ in range(10):\n",
        "    x = res_block(x, 256)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(L2_REG))(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(L2_REG))(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "cnn_model = keras.Model(inputs, outputs)\n",
        "cnn_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(INIT_LR),\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        keras.metrics.TopKCategoricalAccuracy(k=2, name='top2')\n",
        "    ]\n",
        ")\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train CNN \u2014 Phase 1 + Phase 2 (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_ckpt = f'{MODEL_DIR}/connect4_cnn_v2_best.keras'\n",
        "cnn_callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(cnn_ckpt, monitor='val_accuracy', save_best_only=True),\n",
        "    keras.callbacks.LearningRateScheduler(warmup_cosine_lr),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=EARLY_STOP_PATIENCE,\n",
        "        min_delta=0.002, restore_best_weights=True\n",
        "    ),\n",
        "]\n",
        "\n",
        "print('Training CNN Phase 1...')\n",
        "cnn_history = cnn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE1, callbacks=cnn_callbacks)\n",
        "cnn_val_acc = max(cnn_history.history['val_accuracy'])\n",
        "print(f'CNN Phase 1 best val_accuracy: {cnn_val_acc:.4f}')\n",
        "\n",
        "if cnn_val_acc < TARGET_VAL_ACC:\n",
        "    print(f'Phase 2: Fine-tuning (val_acc {cnn_val_acc:.2%} < {TARGET_VAL_ACC:.0%})...')\n",
        "    cnn_model.load_weights(cnn_ckpt)\n",
        "    cnn_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(FINETUNE_LR),\n",
        "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy(name='accuracy'), keras.metrics.TopKCategoricalAccuracy(k=2, name='top2')]\n",
        "    )\n",
        "    cnn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_FINETUNE)\n",
        "    cnn_model.save(cnn_ckpt)\n",
        "\n",
        "cnn_metrics = cnn_model.evaluate(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Final CNN v2 Model & Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_v2_final = f'{MODEL_DIR}/connect4_cnn_v2_final.keras'\n",
        "cnn_compat_final = f'{MODEL_DIR}/connect4_cnn_final.keras'\n",
        "cnn_model.save(cnn_v2_final)\n",
        "cnn_model.save(cnn_compat_final)\n",
        "\n",
        "print('='*60)\n",
        "print('DELIVERABLES SAVED')\n",
        "print('='*60)\n",
        "print(f'CNN v2 best:  {cnn_ckpt}')\n",
        "print(f'CNN v2 final: {cnn_v2_final}')\n",
        "print(f'Compatibility alias: {cnn_compat_final}')\n",
        "print('='*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}