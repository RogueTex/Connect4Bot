{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect 4 — Combine Datasets & Deduplicate\n",
    "\n",
    "Load all datasets (yours + Sebastian's + Additional + **Professor MCTS7500**), keep only **unique board positions** (majority vote for conflicting moves), and save a combined dataset ready for CNN and Transformer training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Drive & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Using Google Drive\n",
      "Drive mounted\n",
      "Output dir: /content/drive/MyDrive/Connect4_Combined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = os.getcwd()\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE = '/content/drive/MyDrive'\n",
    "    print('Using Google Drive')\n",
    "except Exception as e:\n",
    "    print('Drive mount skipped, using local:', type(e).__name__)\n",
    "\n",
    "# Output for combined dataset\n",
    "OUTPUT_DIR = f'{BASE}/Connect4_Combined'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f'{OUTPUT_DIR}/datasets', exist_ok=True)\n",
    "\n",
    "print('Drive mounted')\n",
    "print(f'Output dir: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Paths — Update if yours differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ UPDATE THESE PATHS IF YOUR FOLDER STRUCTURE DIFFERS ============\n",
    "\n",
    "DATASET_PATHS = [\n",
    "    # Your Fast\n",
    "    f'{BASE}/Connect4_FAST/datasets/connect4_fast_5000.npz',\n",
    "    f'{BASE}/Connect4_FAST/datasets/connect4_high_depth.npz',\n",
    "    # Your Deep Search\n",
    "    f'{BASE}/Connect4_DeepSearch/datasets/connect4_deep_search.npz',\n",
    "    # Sebastian's Fast\n",
    "    f'{BASE}/Connect4FastSearch_Sebastian/Connect4_FAST/datasets/connect4_fast_5000.npz',\n",
    "    # Sebastian's Deep Search\n",
    "    f'{BASE}/Connect4DeepSearch_Sebastian/Connect4_DeepSearch/datasets/connect4_deep_search.npz',\n",
    "]\n",
    "\n",
    "# Additional DataGenerator (best_20k) — upload to Drive or point to local path\n",
    "# Option A: If uploaded to Drive under Connect4_AdditionalData or similar\n",
    "ADDITIONAL_X = f'{BASE}/Connect4_AdditionalData/Additional_DataGenerator/best_20k_X.npy'\n",
    "ADDITIONAL_Y = f'{BASE}/Connect4_AdditionalData/Additional_DataGenerator/best_20k_Y.npy'\n",
    "\n",
    "# Option B: If you upload the .npy files to Colab session storage (use file picker below)\n",
    "# ADDITIONAL_X = '/content/best_20k_X.npy'\n",
    "# ADDITIONAL_Y = '/content/best_20k_Y.npy'\n",
    "\n",
    "USE_ADDITIONAL = True  # Set False to skip best_20k if not available\n",
    "\n",
    "# Professor's MCTS7500 dataset (option-a encoding: 6x7 +1/-1/0)\n",
    "PROFESSOR_CANDIDATES = [f'{BASE}/mcts7500_pool.pickle', 'mcts7500_pool.pickle',\n",
    "                       os.path.join(os.getcwd(), 'mcts7500_pool.pickle')]\n",
    "PROFESSOR_PICKLE = next((p for p in PROFESSOR_CANDIDATES if os.path.exists(p)), PROFESSOR_CANDIDATES[0])\n",
    "USE_PROFESSOR = True  # Set False to skip professor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1450610228.py:39: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  prof = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  connect4_fast_5000.npz: 254,230 samples\n",
      "  connect4_high_depth.npz: 3,997 samples\n",
      "  connect4_deep_search.npz: 59,948 samples\n",
      "  connect4_fast_5000.npz: 254,228 samples\n",
      "  connect4_deep_search.npz: 60,954 samples\n",
      "  professor_MCTS7500: 265,620 samples\n",
      "\n",
      "Skipped (not found):\n",
      "  /content/drive/MyDrive/Connect4_AdditionalData/best_20k_X.npy / /content/drive/MyDrive/Connect4_AdditionalData/best_20k_Y.npy\n"
     ]
    }
   ],
   "source": [
    "def load_npz(path):\n",
    "    \"\"\"Load X and y from npz. Handles X_train/y_train or X/y_move.\"\"\"\n",
    "    data = np.load(path)\n",
    "    if 'X_train' in data:\n",
    "        X = data['X_train']\n",
    "        y = data['y_train']\n",
    "    elif 'X' in data:\n",
    "        X = data['X']\n",
    "        y = data['y_move'] if 'y_move' in data else data['y']\n",
    "    else:\n",
    "        raise KeyError(f'Unknown keys: {list(data.keys())}')\n",
    "    return X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "all_X, all_y = [], []\n",
    "loaded, skipped = [], []\n",
    "\n",
    "for p in DATASET_PATHS:\n",
    "    if os.path.exists(p):\n",
    "        X, y = load_npz(p)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "        loaded.append((p.split('/')[-1], len(X)))\n",
    "    else:\n",
    "        skipped.append(p)\n",
    "\n",
    "# Additional (best_20k)\n",
    "if USE_ADDITIONAL and os.path.exists(ADDITIONAL_X) and os.path.exists(ADDITIONAL_Y):\n",
    "    X_add = np.load(ADDITIONAL_X).astype(np.float32)\n",
    "    y_add = np.load(ADDITIONAL_Y).astype(np.int64)\n",
    "    all_X.append(X_add)\n",
    "    all_y.append(y_add)\n",
    "    loaded.append(('best_20k (additional)', len(X_add)))\n",
    "elif USE_ADDITIONAL:\n",
    "    skipped.append(f'{ADDITIONAL_X} / {ADDITIONAL_Y}')\n",
    "\n",
    "# Professor's MCTS7500 (option-a -> option-b conversion)\n",
    "if USE_PROFESSOR and os.path.exists(PROFESSOR_PICKLE):\n",
    "    with open(PROFESSOR_PICKLE, 'rb') as f:\n",
    "        prof = pickle.load(f)\n",
    "    board_x = np.array(prof['board_x'], dtype=np.float32)\n",
    "    play_y = np.array(prof['play_y'], dtype=np.int64)\n",
    "    def option_a_to_b(b):\n",
    "        out = np.zeros((6, 7, 2), dtype=np.float32)\n",
    "        out[:, :, 0] = (b == 1).astype(np.float32)\n",
    "        out[:, :, 1] = (b == -1).astype(np.float32)\n",
    "        return out\n",
    "    X_prof = np.array([option_a_to_b(b) for b in board_x], dtype=np.float32)\n",
    "    all_X.append(X_prof)\n",
    "    all_y.append(play_y)\n",
    "    loaded.append(('professor_MCTS7500', len(X_prof)))\n",
    "elif USE_PROFESSOR:\n",
    "    skipped.append(PROFESSOR_PICKLE)\n",
    "\n",
    "print('Loaded:')\n",
    "for name, n in loaded:\n",
    "    print(f'  {name}: {n:,} samples')\n",
    "if skipped:\n",
    "    print('\\nSkipped (not found):')\n",
    "    for p in skipped:\n",
    "        print(f'  {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatenate & Deduplicate (Unique Boards Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total before dedup: 898,977 samples\n",
      "  Processed 100,000 / 898,977\n",
      "  Processed 200,000 / 898,977\n",
      "  Processed 300,000 / 898,977\n",
      "  Processed 400,000 / 898,977\n",
      "  Processed 500,000 / 898,977\n",
      "  Processed 600,000 / 898,977\n",
      "  Processed 700,000 / 898,977\n",
      "  Processed 800,000 / 898,977\n",
      "\n",
      "After dedup: 680,166 unique boards\n",
      "Positions with conflicting moves (used majority): 19,830\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all\n",
    "X_all = np.concatenate(all_X, axis=0)\n",
    "y_all = np.concatenate(all_y, axis=0)\n",
    "print(f'Total before dedup: {len(X_all):,} samples')\n",
    "\n",
    "# Deduplicate by board (hash) — keep one per unique position, majority vote for move\n",
    "from collections import Counter\n",
    "\n",
    "board_to_moves = defaultdict(list)  # hash -> list of (X, y)\n",
    "\n",
    "for i in range(len(X_all)):\n",
    "    key = X_all[i].tobytes()\n",
    "    board_to_moves[key].append((X_all[i], int(y_all[i])))\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(f'  Processed {i+1:,} / {len(X_all):,}')\n",
    "\n",
    "# For each unique board, take majority vote on move\n",
    "X_unique = []\n",
    "y_unique = []\n",
    "conflicts = 0\n",
    "\n",
    "for key, entries in board_to_moves.items():\n",
    "    boards = [e[0] for e in entries]\n",
    "    moves = [e[1] for e in entries]\n",
    "    if len(set(moves)) > 1:\n",
    "        conflicts += 1\n",
    "    majority_move = Counter(moves).most_common(1)[0][0]\n",
    "    X_unique.append(boards[0])  # same board for all entries\n",
    "    y_unique.append(majority_move)\n",
    "\n",
    "X_unique = np.array(X_unique, dtype=np.float32)\n",
    "y_unique = np.array(y_unique, dtype=np.int64)\n",
    "\n",
    "print(f'\\nAfter dedup: {len(X_unique):,} unique boards')\n",
    "print(f'Positions with conflicting moves (used majority): {conflicts:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shuffle & Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "idx = np.random.permutation(len(X_unique))\n",
    "X_unique = X_unique[idx]\n",
    "y_unique = y_unique[idx]\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "n = len(X_unique)\n",
    "n_test = int(n * TEST_SPLIT)\n",
    "n_val = int(n * VAL_SPLIT)\n",
    "n_train = n - n_test - n_val\n",
    "\n",
    "X_test = X_unique[:n_test]\n",
    "y_test = y_unique[:n_test]\n",
    "X_val = X_unique[n_test:n_test+n_val]\n",
    "y_val = y_unique[n_test:n_test+n_val]\n",
    "X_train = X_unique[n_test+n_val:]\n",
    "y_train = y_unique[n_test+n_val:]\n",
    "\n",
    "print('Split:')\n",
    "print(f'  Train: {X_train.shape[0]:,}')\n",
    "print(f'  Val:   {X_val.shape[0]:,}')\n",
    "print(f'  Test:  {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = f'{OUTPUT_DIR}/datasets/connect4_combined_unique.npz'\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_PATH,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "size_mb = os.path.getsize(DATASET_PATH) / (1024**2)\n",
    "print(f'Saved: {DATASET_PATH}')\n",
    "print(f'Size: {size_mb:.1f} MB')\n",
    "print('\\nKeys: X_train, y_train, X_val, y_val, X_test, y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Load Test (for your training notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify — use this pattern in your CNN/Transformer training notebook\n",
    "data = np.load(DATASET_PATH)\n",
    "X_tr = data['X_train']\n",
    "y_tr = data['y_train']\n",
    "X_v = data['X_val']\n",
    "y_v = data['y_val']\n",
    "X_te = data['X_test']\n",
    "y_te = data['y_test']\n",
    "\n",
    "print(f'X_train: {X_tr.shape} (6x7x2)')\n",
    "print(f'y_train: {y_tr.shape} (column 0-6)')\n",
    "print(f'Move distribution: {np.bincount(y_tr.astype(int), minlength=7)}')\n",
    "print('\\nReady for CNN and Transformer training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload best_20k if not on Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you need to upload best_20k_X.npy and best_20k_Y.npy from your computer.\n",
    "# Then set USE_ADDITIONAL=True and ADDITIONAL_X='/content/best_20k_X.npy', ADDITIONAL_Y='/content/best_20k_Y.npy'\n",
    "# and re-run from cell 3.\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print('Upload best_20k_X.npy and best_20k_Y.npy (one at a time)')\n",
    "uploaded = files.upload()  # Will prompt for files\n",
    "# Files save to /content/ — then set ADDITIONAL_X = '/content/best_20k_X.npy', etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
