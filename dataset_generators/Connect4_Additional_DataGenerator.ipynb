{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect 4 Additional Data Generator (Notebook Format)\n",
    "\n",
    "**Speed-optimized MCTS data generator** — same format as your existing notebooks.\n",
    "\n",
    "## Features\n",
    "- **Numba JIT** for fast rollouts (~10–20x speedup)\n",
    "- **Zero-leakage deduplication** across 4 board transformations\n",
    "- **6×7×2 encoding** — same as Connect4_DeepSearch and Connect4_Dataset_Generator\n",
    "- **Output format** — `npz` with `X_train`, `y_train` (DeepSearch style) and optional `X`, `y_move`, `y_result`, `turns` (Dataset_Generator style)\n",
    "\n",
    "## Output formats (choose one or both)\n",
    "1. **DeepSearch style**: `X_train`, `y_train` in `.npz`\n",
    "2. **Dataset_Generator style**: `X`, `y_move`, `y_result`, `turns` in `.npz` + train/val/test splits\n",
    "\n",
    "## Usage\n",
    "1. Run cells in order.\n",
    "2. Or use the **Load existing .npy** cell if you already have `best_20k_X.npy` / `best_20k_Y.npy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment setup & directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ---------- Auto-detect Colab vs Local ----------\n",
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_DIR = '/content/drive/MyDrive/Connect4_AdditionalData'\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    # Local -- save in Additional_DataGenerator or current dir\n",
    "    PROJECT_DIR = os.path.join(os.getcwd(), 'Additional_DataGenerator')\n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        PROJECT_DIR = os.getcwd()\n",
    "    print(\"Running locally\")\n",
    "\n",
    "DATASET_DIR = os.path.join(PROJECT_DIR, 'datasets')\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_DIR, 'checkpoints')\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project dir: {PROJECT_DIR}\")\n",
    "print(f\"Dataset dir: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "NUM_GAMES = 10000\n",
    "MCTS_ROLLOUTS = 10000\n",
    "RANDOM_OPENING_MOVES = 6\n",
    "NUM_WORKERS = None  # None = auto (cpu_count)\n",
    "SAVE_PREFIX = \"fast_10k\"\n",
    "CHECKPOINT_EVERY = 500\n",
    "\n",
    "# Output format: 'both' | 'deepsearch' | 'dataset_generator'\n",
    "# - deepsearch: X_train, y_train only (npz)\n",
    "# - dataset_generator: X, y_move, y_result, turns + train/val/test splits\n",
    "# - both: save in both formats\n",
    "OUTPUT_FORMAT = 'both'\n",
    "\n",
    "SEED = 42\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Games: {NUM_GAMES:,} | MCTS rollouts: {MCTS_ROLLOUTS:,} | Opening moves: 0-{RANDOM_OPENING_MOVES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import hashlib\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "    HAS_NUMBA = True\n",
    "except ImportError:\n",
    "    HAS_NUMBA = False\n",
    "    def njit(*args, **kwargs):\n",
    "        if args and callable(args[0]):\n",
    "            return args[0]\n",
    "        return lambda f: f\n",
    "\n",
    "print(f\"Numba: {'YES' if HAS_NUMBA else 'NO (install numba for speed)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Numba JIT game engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def find_row(board, col):\n",
    "    for r in range(5, -1, -1):\n",
    "        if board[r, col] == 0:\n",
    "            return r\n",
    "    return -1\n",
    "\n",
    "@njit(cache=True)\n",
    "def find_legal(board):\n",
    "    moves = np.empty(7, dtype=np.int32)\n",
    "    n = 0\n",
    "    for c in range(7):\n",
    "        if board[0, c] == 0:\n",
    "            moves[n] = c\n",
    "            n += 1\n",
    "    return moves[:n]\n",
    "\n",
    "@njit(cache=True)\n",
    "def check_win(board, row, col):\n",
    "    p = board[row, col]\n",
    "    if p == 0:\n",
    "        return False\n",
    "    if row <= 2:\n",
    "        if board[row+1,col]==p and board[row+2,col]==p and board[row+3,col]==p:\n",
    "            return True\n",
    "    count = 0\n",
    "    for c in range(7):\n",
    "        if board[row, c] == p:\n",
    "            count += 1\n",
    "            if count >= 4:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "    count = 0\n",
    "    sr = row - min(row, col)\n",
    "    sc = col - min(row, col)\n",
    "    while sr < 6 and sc < 7:\n",
    "        if board[sr, sc] == p:\n",
    "            count += 1\n",
    "            if count >= 4:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "        sr += 1\n",
    "        sc += 1\n",
    "    count = 0\n",
    "    sr = row + min(5 - row, col)\n",
    "    sc = col - min(5 - row, col)\n",
    "    while sr >= 0 and sc < 7:\n",
    "        if board[sr, sc] == p:\n",
    "            count += 1\n",
    "            if count >= 4:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "        sr -= 1\n",
    "        sc += 1\n",
    "    return False\n",
    "\n",
    "@njit(cache=True)\n",
    "def try_win(board, col, player):\n",
    "    row = find_row(board, col)\n",
    "    if row < 0:\n",
    "        return False\n",
    "    board[row, col] = player\n",
    "    won = check_win(board, row, col)\n",
    "    board[row, col] = 0\n",
    "    return won\n",
    "\n",
    "@njit(cache=True)\n",
    "def rollout_jit(board, next_player):\n",
    "    b = board.copy()\n",
    "    p = next_player\n",
    "    for _ in range(42):\n",
    "        legal = find_legal(b)\n",
    "        if len(legal) == 0:\n",
    "            return 0\n",
    "        opp = -p\n",
    "        chosen = np.int32(-1)\n",
    "        for i in range(len(legal)):\n",
    "            if try_win(b, legal[i], p):\n",
    "                return p\n",
    "        for i in range(len(legal)):\n",
    "            if try_win(b, legal[i], opp):\n",
    "                chosen = legal[i]\n",
    "                break\n",
    "        if chosen == -1:\n",
    "            has_center = False\n",
    "            for i in range(len(legal)):\n",
    "                if legal[i] == 3:\n",
    "                    has_center = True\n",
    "                    break\n",
    "            if has_center and np.random.random() < 0.2:\n",
    "                chosen = np.int32(3)\n",
    "            else:\n",
    "                chosen = legal[np.random.randint(len(legal))]\n",
    "        row = find_row(b, chosen)\n",
    "        b[row, chosen] = p\n",
    "        if check_win(b, row, chosen):\n",
    "            return p\n",
    "        p = opp\n",
    "    return 0\n",
    "\n",
    "print(\"Game engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: MCTS & data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts_fast(board, color_val, nsteps):\n",
    "    legal0 = find_legal(board)\n",
    "    for c in legal0:\n",
    "        if try_win(board, int(c), color_val):\n",
    "            return int(c)\n",
    "    opp = -color_val\n",
    "    allowed = []\n",
    "    for c in legal0:\n",
    "        col = int(c)\n",
    "        row = int(find_row(board, col))\n",
    "        board[row, col] = color_val\n",
    "        opp_legal = find_legal(board)\n",
    "        loses = False\n",
    "        for oc in opp_legal:\n",
    "            if try_win(board, int(oc), opp):\n",
    "                loses = True\n",
    "                break\n",
    "        board[row, col] = 0\n",
    "        if not loses:\n",
    "            allowed.append(col)\n",
    "    if not allowed:\n",
    "        allowed = [int(c) for c in legal0]\n",
    "    root_key = board.tobytes()\n",
    "    md = {root_key: [0, 0]}\n",
    "    _sqrt = math.sqrt\n",
    "    _log = math.log\n",
    "    for _ in range(nsteps):\n",
    "        cv = color_val\n",
    "        b = board.copy()\n",
    "        path = [root_key]\n",
    "        while True:\n",
    "            legal = find_legal(b)\n",
    "            n_legal = len(legal)\n",
    "            if n_legal == 0:\n",
    "                for key in path:\n",
    "                    md[key][0] += 1\n",
    "                break\n",
    "            keys = [None] * n_legal\n",
    "            for i in range(n_legal):\n",
    "                col = int(legal[i])\n",
    "                row = int(find_row(b, col))\n",
    "                b[row, col] = cv\n",
    "                k = b.tobytes()\n",
    "                keys[i] = k\n",
    "                if k not in md:\n",
    "                    md[k] = [0, 0]\n",
    "                b[row, col] = 0\n",
    "            parent_n = md[path[-1]][0]\n",
    "            best_ucb = -1e30\n",
    "            best_i = 0\n",
    "            if parent_n == 0:\n",
    "                best_i = 0\n",
    "            else:\n",
    "                log_p = _log(parent_n)\n",
    "                for i in range(n_legal):\n",
    "                    nd = md[keys[i]]\n",
    "                    if nd[0] == 0:\n",
    "                        best_i = i\n",
    "                        break\n",
    "                    ucb = nd[1] / nd[0] + 2.0 * _sqrt(log_p / nd[0])\n",
    "                    if ucb > best_ucb:\n",
    "                        best_ucb = ucb\n",
    "                        best_i = i\n",
    "            chosen_col = int(legal[best_i])\n",
    "            row = int(find_row(b, chosen_col))\n",
    "            b[row, chosen_col] = cv\n",
    "            path.append(keys[best_i])\n",
    "            if check_win(b, row, chosen_col):\n",
    "                winner = cv\n",
    "                for j, key in enumerate(path):\n",
    "                    md[key][0] += 1\n",
    "                    if winner == color_val:\n",
    "                        md[key][1] += 1 if j % 2 == 1 else -1\n",
    "                    else:\n",
    "                        md[key][1] += -1 if j % 2 == 1 else 1\n",
    "                break\n",
    "            cv = -cv\n",
    "            if md[keys[best_i]][0] == 0:\n",
    "                result = int(rollout_jit(b, cv))\n",
    "                for j, key in enumerate(path):\n",
    "                    md[key][0] += 1\n",
    "                    if result == 0:\n",
    "                        pass\n",
    "                    elif result == color_val:\n",
    "                        md[key][1] += 1 if j % 2 == 1 else -1\n",
    "                    else:\n",
    "                        md[key][1] += -1 if j % 2 == 1 else 1\n",
    "                break\n",
    "    best_val = -1e30\n",
    "    best_col = allowed[0]\n",
    "    for col in allowed:\n",
    "        row = int(find_row(board, col))\n",
    "        board[row, col] = color_val\n",
    "        k = board.tobytes()\n",
    "        board[row, col] = 0\n",
    "        if k in md and md[k][0] > 0:\n",
    "            val = md[k][1] / md[k][0]\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_col = col\n",
    "    return best_col\n",
    "\n",
    "\n",
    "def board_to_canonical(board, current_player_val):\n",
    "    canonical = np.zeros((6, 7, 2), dtype=np.float32)\n",
    "    canonical[:, :, 0] = (board == current_player_val).astype(np.float32)\n",
    "    canonical[:, :, 1] = (board == -current_player_val).astype(np.float32)\n",
    "    return canonical\n",
    "\n",
    "\n",
    "def play_single_game(args):\n",
    "    game_id, mcts_rollouts, random_opening_moves = args\n",
    "    board = np.zeros((6, 7), dtype=np.int8)\n",
    "    cv = np.int8(1)\n",
    "    game_data = []\n",
    "    n_random = random.randint(0, random_opening_moves)\n",
    "    for _ in range(n_random):\n",
    "        legal = find_legal(board)\n",
    "        if len(legal) == 0:\n",
    "            break\n",
    "        col = int(random.choice(legal))\n",
    "        row = int(find_row(board, col))\n",
    "        board[row, col] = cv\n",
    "        if check_win(board, row, col):\n",
    "            return []\n",
    "        cv = np.int8(-cv)\n",
    "    recorded = 0\n",
    "    winner = 0\n",
    "    while True:\n",
    "        legal = find_legal(board)\n",
    "        if len(legal) == 0:\n",
    "            winner = 0\n",
    "            break\n",
    "        move = mcts_fast(board, int(cv), mcts_rollouts)\n",
    "        canonical = board_to_canonical(board, int(cv))\n",
    "        current_player = int(cv)\n",
    "        game_data.append((canonical, move, {\n",
    "            'game_id': game_id,\n",
    "            'move_number': recorded,\n",
    "            'current_player': current_player,\n",
    "        }))\n",
    "        recorded += 1\n",
    "        row = int(find_row(board, move))\n",
    "        board[row, move] = cv\n",
    "        if check_win(board, row, move):\n",
    "            winner = int(cv)\n",
    "            break\n",
    "        cv = np.int8(-cv)\n",
    "    for i, (c, m, meta) in enumerate(game_data):\n",
    "        meta['winner'] = winner\n",
    "        meta['result'] = 1.0 if winner == meta['current_player'] else (0.0 if winner != 0 else 0.5)\n",
    "    return game_data\n",
    "\n",
    "print(\"MCTS & play_single_game loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Deduplication & verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_board(board):\n",
    "    return hashlib.md5(board.tobytes()).hexdigest()\n",
    "\n",
    "def get_all_equivalent_boards(board, move):\n",
    "    equivalents = []\n",
    "    equivalents.append((board.copy(), move, 'original'))\n",
    "    mirror = board[:, ::-1, :].copy()\n",
    "    equivalents.append((mirror, 6 - move, 'mirror'))\n",
    "    flip = board[:, :, [1, 0]].copy()\n",
    "    equivalents.append((flip, move, 'perspective_flip'))\n",
    "    mirror_flip = mirror[:, :, [1, 0]].copy()\n",
    "    equivalents.append((mirror_flip, 6 - move, 'mirror_and_flip'))\n",
    "    return equivalents\n",
    "\n",
    "def normalize_to_canonical_form(board, move):\n",
    "    equivalents = get_all_equivalent_boards(board, move)\n",
    "    min_hash = None\n",
    "    canonical = None\n",
    "    for equiv_board, equiv_move, transform in equivalents:\n",
    "        h = hash_board(equiv_board)\n",
    "        if min_hash is None or h < min_hash:\n",
    "            min_hash = h\n",
    "            canonical = (equiv_board, equiv_move, transform)\n",
    "    return canonical\n",
    "\n",
    "def deduplicate_all_transformations(raw_data):\n",
    "    print(\"  Phase 1: Normalizing to canonical form...\")\n",
    "    canonical_to_entries = defaultdict(list)\n",
    "    for idx, (board, label, metadata) in enumerate(raw_data):\n",
    "        if idx % 50000 == 0 and idx > 0:\n",
    "            print(f\"    {idx:,} / {len(raw_data):,}...\")\n",
    "        canon_board, canon_move, transform = normalize_to_canonical_form(board, label)\n",
    "        canon_hash = hash_board(canon_board)\n",
    "        canonical_to_entries[canon_hash].append({\n",
    "            'board': canon_board, 'label': canon_move,\n",
    "            'metadata': metadata, 'transform': transform,\n",
    "        })\n",
    "    print(f\"  Phase 1 done: {len(raw_data):,} -> {len(canonical_to_entries):,} unique\")\n",
    "    print(\"  Phase 2: Majority voting...\")\n",
    "    clean_data = []\n",
    "    for canon_hash, entries in canonical_to_entries.items():\n",
    "        if len(entries) == 1:\n",
    "            e = entries[0]\n",
    "            clean_data.append((e['board'], e['label'], e['metadata']))\n",
    "        else:\n",
    "            labels = [e['label'] for e in entries]\n",
    "            unique_labels = set(labels)\n",
    "            if len(unique_labels) <= 2:\n",
    "                label_counts = Counter(labels)\n",
    "                majority_label = label_counts.most_common(1)[0][0]\n",
    "                for e in entries:\n",
    "                    if e['label'] == majority_label:\n",
    "                        clean_data.append((e['board'], e['label'], e['metadata']))\n",
    "                        break\n",
    "            # else: skip conflicts\n",
    "    print(f\"  Final: {len(clean_data):,} samples\")\n",
    "    return clean_data\n",
    "\n",
    "def warmup_numba():\n",
    "    if not HAS_NUMBA:\n",
    "        print(\"[WARMUP] Numba not available\")\n",
    "        return\n",
    "    print(\"[WARMUP] Compiling Numba...\", end=\" \", flush=True)\n",
    "    t0 = time.time()\n",
    "    b = np.zeros((6, 7), dtype=np.int8)\n",
    "    _ = find_legal(b)\n",
    "    _ = rollout_jit(b, np.int8(1))\n",
    "    print(f\"done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(\"Deduplication & verification loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run generation (or load existing .npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Option A: Generate new data ----------\n",
    "GENERATE_NEW = True  # Set False to only load existing .npy files\n",
    "\n",
    "if GENERATE_NEW:\n",
    "    warmup_numba()\n",
    "    nw = NUM_WORKERS or max(1, cpu_count())\n",
    "    args_list = [(i, MCTS_ROLLOUTS, RANDOM_OPENING_MOVES) for i in range(NUM_GAMES)]\n",
    "    all_data = []\n",
    "    start = time.time()\n",
    "    with Pool(nw) as pool:\n",
    "        for game_data in pool.imap_unordered(play_single_game, args_list, chunksize=1):\n",
    "            all_data.extend(game_data)\n",
    "            if len(all_data) % 50000 == 0 and len(all_data) > 0:\n",
    "                print(f\"  Samples: {len(all_data):,}\")\n",
    "    print(f\"Raw: {len(all_data):,} samples in {time.time()-start:.1f}s\")\n",
    "    clean_data = deduplicate_all_transformations(all_data)\n",
    "    X = np.array([b for b, _, _ in clean_data], dtype=np.float32)\n",
    "    Y = np.array([m for _, m, _ in clean_data], dtype=np.int64)\n",
    "    metadata = [m for _, _, m in clean_data]\n",
    "else:\n",
    "    # ---------- Option B: Load existing .npy (e.g. best_20k_X.npy, best_20k_Y.npy) ----------\n",
    "    LOAD_PATH = os.path.join(PROJECT_DIR, 'best_20k')  # prefix without _X / _Y\n",
    "    X = np.load(LOAD_PATH + '_X.npy')\n",
    "    Y = np.load(LOAD_PATH + '_Y.npy')\n",
    "    metadata = []\n",
    "    if os.path.exists(LOAD_PATH + '_metadata.pkl'):\n",
    "        with open(LOAD_PATH + '_metadata.pkl', 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "    else:\n",
    "        metadata = [{'move_number': i % 21, 'result': 0.5} for i in range(len(X))]\n",
    "    print(f\"Loaded: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "print(f\"X shape: {X.shape} | Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Build dataset in notebook format (X_train, y_train, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same format as Connect4_DeepSearch and Connect4_Dataset_Generator\n",
    "n = len(X)\n",
    "y_move = Y.astype(np.int8) if Y.dtype != np.int8 else Y\n",
    "turns = np.array([m.get('move_number', 0) for m in metadata], dtype=np.int8) if metadata else np.zeros(n, dtype=np.int8)\n",
    "y_result = np.array([m.get('result', 0.5) for m in metadata], dtype=np.float32) if metadata else np.full(n, 0.5, dtype=np.float32)\n",
    "\n",
    "# Shuffle\n",
    "idx = np.random.permutation(n)\n",
    "X = X[idx]\n",
    "y_move = y_move[idx]\n",
    "turns = turns[idx]\n",
    "y_result = y_result[idx]\n",
    "if metadata:\n",
    "    metadata = [metadata[i] for i in idx]\n",
    "\n",
    "print(f\"Prepared: X={X.shape}, y_move={y_move.shape}, y_result={y_result.shape}, turns={turns.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Save in notebook format (npz + optional splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Save DeepSearch-style (X_train, y_train)\n",
    "if OUTPUT_FORMAT in ('both', 'deepsearch'):\n",
    "    npz_path = os.path.join(DATASET_DIR, 'connect4_additional_data.npz')\n",
    "    np.savez_compressed(npz_path, X_train=X, y_train=y_move)\n",
    "    print(f\"Saved (DeepSearch style): {npz_path}\")\n",
    "\n",
    "# Save Dataset_Generator-style (X, y_move, y_result, turns) + train/val/test\n",
    "if OUTPUT_FORMAT in ('both', 'dataset_generator'):\n",
    "    full_path = os.path.join(DATASET_DIR, 'connect4_additional_full.npz')\n",
    "    np.savez_compressed(full_path, X=X, y_move=y_move, y_result=y_result, turns=turns)\n",
    "    print(f\"Saved (Dataset_Generator style): {full_path}\")\n",
    "    X_tr, X_tmp, y_m_tr, y_m_tmp, y_r_tr, y_r_tmp, t_tr, t_tmp = train_test_split(\n",
    "        X, y_move, y_result, turns, test_size=0.2, random_state=SEED)\n",
    "    X_val, X_te, y_m_val, y_m_te, y_r_val, y_r_te, t_val, t_te = train_test_split(\n",
    "        X_tmp, y_m_tmp, y_r_tmp, t_tmp, test_size=0.5, random_state=SEED)\n",
    "    np.savez_compressed(os.path.join(DATASET_DIR, 'train.npz'), X=X_tr, y_move=y_m_tr, y_result=y_r_tr, turns=t_tr)\n",
    "    np.savez_compressed(os.path.join(DATASET_DIR, 'val.npz'), X=X_val, y_move=y_m_val, y_result=y_r_val, turns=t_val)\n",
    "    np.savez_compressed(os.path.join(DATASET_DIR, 'test.npz'), X=X_te, y_move=y_m_te, y_result=y_r_te, turns=t_te)\n",
    "    print(f\"Saved train/val/test splits to {DATASET_DIR}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Optional merge with existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with existing dataset (e.g. from DeepSearch or Dataset_Generator)\n",
    "EXISTING_DATASET_PATH = None  # e.g. 'Connect4_DeepSearch_Output/datasets/connect4_deep_search.npz'\n",
    "\n",
    "if EXISTING_DATASET_PATH and os.path.exists(EXISTING_DATASET_PATH):\n",
    "    existing = np.load(EXISTING_DATASET_PATH)\n",
    "    X_ex = existing['X_train'] if 'X_train' in existing else existing['X']\n",
    "    y_ex = existing['y_train'] if 'y_train' in existing else existing['y_move']\n",
    "    X_merged = np.concatenate([X_ex, X], axis=0)\n",
    "    y_merged = np.concatenate([y_ex, y_move], axis=0)\n",
    "    idx = np.random.permutation(len(X_merged))\n",
    "    X_merged = X_merged[idx]\n",
    "    y_merged = y_merged[idx]\n",
    "    merged_path = os.path.join(DATASET_DIR, 'connect4_merged.npz')\n",
    "    np.savez_compressed(merged_path, X_train=X_merged, y_train=y_merged)\n",
    "    print(f\"Merged: {merged_path} | X={X_merged.shape}\")\n",
    "else:\n",
    "    print(\"No existing path set. Set EXISTING_DATASET_PATH to merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load in your training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DeepSearch-style\n",
    "data = np.load(os.path.join(DATASET_DIR, 'connect4_additional_data.npz'))\n",
    "X_train = data['X_train']  # (N, 6, 7, 2)\n",
    "y_train = data['y_train']  # (N,) column 0-6\n",
    "print(f\"X: {X_train.shape} | y: {y_train.shape}\")\n",
    "\n",
    "# Load Dataset_Generator-style (if you saved it)\n",
    "# data = np.load(os.path.join(DATASET_DIR, 'connect4_additional_full.npz'))\n",
    "# X, y_move, y_result, turns = data['X'], data['y_move'], data['y_result'], data['turns']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
